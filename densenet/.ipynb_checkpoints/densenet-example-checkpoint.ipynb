{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e081b7",
   "metadata": {},
   "source": [
    "# Sparsifying DenseNet121 from Scratch (Flower102)\n",
    "\n",
    "In this example, we will demonstrate how to sparsify an image classification model from scratch using SparseML's PyTorch integration. We train and prune [DenseNet121](https://pytorch.org/vision/main/models/generated/torchvision.models.densenet121.html) on the downstream [Oxford Flower 102 dataset](https://pytorch.org/vision/main/generated/torchvision.datasets.Flowers102.html#:~:text=Oxford%20102%20Flower%20is%20an,scale%2C%20pose%20and%20light%20variations) using the Global Magnitude Pruning algorithm. \n",
    "\n",
    "## Agenda\n",
    "\n",
    "There are a few steps:\n",
    "\n",
    " 1. Setup the dataset\n",
    " 2. Setup the PyTorch training loop\n",
    " 3. Train a dense version of DenseNet121\n",
    " 4. Run the GMP pruning algorithm and QAT quantization algorithm on the dense model\n",
    " \n",
    "## Installation\n",
    "\n",
    "Install SparseML with `pip`:\n",
    "\n",
    "```\n",
    "pip install sparseml[torchvision]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad80edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sparseml\n",
    "import torchvision\n",
    "from sparseml.pytorch.optim import ScheduledModifierManager\n",
    "from sparseml.pytorch.utils import TensorBoardLogger, ModuleExporter, get_prunable_layers, tensor_sparsity\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01779bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cu116\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a128dcc",
   "metadata": {},
   "source": [
    "## **Step 1: Setup Dataset**\n",
    "\n",
    "Oxford 102 Flower is an image classification dataset consisting of 102 flower categories. The flowers were chosen to be flowers commonly occurring in the United Kingdom. Each class consists of between 40 and 258 images. The images have large scale, pose and light variations. In addition, there are categories that have large variations within the category, and several very similar categories.\n",
    "\n",
    "We use the standard PyTorch `datasets` and `dataloaders` to manage the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d180bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to data/flowers-102/102flowers.tgz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763bac85aa6a4428a625456fc126a409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/344862509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_LABELS = 102\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# imagenet transforms\n",
    "imagenet_transform = transforms.Compose([\n",
    "   transforms.Resize(size=256, interpolation=transforms.InterpolationMode.BILINEAR, max_size=None, antialias=None),\n",
    "   transforms.CenterCrop(size=(224, 224)),\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# datasets\n",
    "train_dataset = torchvision.datasets.Flowers102(\n",
    "    root=\"./data\",\n",
    "    split=\"train\",\n",
    "    transform=imagenet_transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "val_dataset = torchvision.datasets.Flowers102(\n",
    "    root=\"./data\",\n",
    "    split=\"val\",\n",
    "    transform=imagenet_transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# dataloaders\n",
    "train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=16)\n",
    "val_loader = DataLoader(val_dataset, BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890cca4a",
   "metadata": {},
   "source": [
    "## Step 2: Setup PyTorch Training Loop\n",
    "\n",
    "We will use this training loop below. This is standard PyTorch functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1b878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "def run_model_one_epoch(model, data_loader, criterion, device, train=False, optimizer=None):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # loop through batches\n",
    "    for step, (inputs, labels) in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # compute loss, run backpropogation\n",
    "        outputs = model(inputs)  # model returns logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # run evaluation\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "        total_correct += torch.sum(predictions == labels).item()\n",
    "        total_predictions += inputs.size(0)\n",
    "\n",
    "    # return loss and evaluation metric\n",
    "    loss = running_loss / (step + 1.0)\n",
    "    accuracy = total_correct / total_predictions\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b385497a",
   "metadata": {},
   "source": [
    "## **Step 3: Train DenseNet121 on Flowers102**\n",
    "\n",
    "First, we will train a dense version of DenseNet121 on the Flowers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d554578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download pre-trained model, setup classification head\n",
    "model = torchvision.models.densenet121(weights=torchvision.models.DenseNet121_Weights.DEFAULT)\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, NUM_LABELS)\n",
    "model.to(device)\n",
    "\n",
    "# setup loss function and optimizer\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=8e-3) # lr will be override by sparseml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb400dc3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Next, we will use SparseML's recipes to set the hyperparameters of training loop. In this case, we will use the following recipe:\n",
    "\n",
    "```yaml\n",
    "# Epoch and Learning-Rate variables\n",
    "num_epochs: 15.0\n",
    "init_lr: 0.001\n",
    "\n",
    "training_modifiers:\n",
    "  - !EpochRangeModifier\n",
    "    start_epoch: 0.0\n",
    "    end_epoch: eval(num_epochs)\n",
    "\n",
    "  - !LearningRateFunctionModifier\n",
    "    final_lr: 0.0\n",
    "    init_lr: eval(init_lr)\n",
    "    lr_func: cosine\n",
    "    start_epoch: 0.0\n",
    "    end_epoch: eval(num_epochs)\n",
    "```\n",
    "\n",
    "As you can see, the recipe includes an `!EpochRangeModifier` and a `!LearningRateFunctionModifier`. These modifiers simply set the number of epochs to train for and the learning rate schedule. As a result, the final model will be dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6a53f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_recipe_path = \"./recipe.dense.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8653c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./recipe.dense.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31aed1b",
   "metadata": {},
   "source": [
    "Next, we use SparseML's `ScheduledModifierManager` to parse and apply the recipe. The `manager.modify` function modifies and wraps the `model` and `optimizer` with the instructions from the recipe. You can use the `model` and `optimizer` just like standard PyTorch objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1749e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ScheduledModifierManager and Optimizer wrapper\n",
    "manager = ScheduledModifierManager.from_yaml(dense_recipe_path)\n",
    "optimizer = manager.modify(model, optimizer, steps_per_epoch=len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee67c917",
   "metadata": {},
   "source": [
    "Kick off the transfer learning loop. Our run reached ~91% validation accuracy after 15 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d175b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "for epoch in range(manager.max_epochs):\n",
    "    # run training loop\n",
    "    epoch_name = f\"{epoch + 1}/{manager.max_epochs}\"\n",
    "    \n",
    "    print(f\"Running Training Epoch {epoch_name}\")\n",
    "    train_loss, train_acc = run_model_one_epoch(model, train_loader, criterion, device, train=True, optimizer=optimizer)\n",
    "    print(f\"Training Epoch: {epoch_name}\\nTraining Loss: {train_loss}\\nTop 1 Acc: {train_acc}\\n\")\n",
    "\n",
    "    # run validation loop\n",
    "    print(f\"Running Validation Epoch {epoch_name}\")\n",
    "    val_loss, val_acc = run_model_one_epoch(model, val_loader, criterion, device)\n",
    "    print(f\"Validation Epoch: {epoch_name}\\nVal Loss: {val_loss}\\nTop 1 Acc: {val_acc}\\n\")\n",
    "\n",
    "# clean up\n",
    "manager.finalize(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb3d14e",
   "metadata": {},
   "source": [
    "Export the model in case we want to reload in the future, so we do not have to rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11fa4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"densenet-models\"\n",
    "exporter = ModuleExporter(model, output_dir=save_dir)\n",
    "exporter.export_pytorch(name=\"dense-model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4532442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7d6309",
   "metadata": {},
   "source": [
    "## Step 4: Prune The Mo\n",
    "\n",
    "With a model trained on Flowers, we are now ready to apply the GMP algorithm to prune the model. The GMP algorithm is an interative pruning algorithm. At the end of each epoch, we identify the lowest magnitude weights (those closest to 0) and remove them from the network starting from an initial level of sparsity until a final level of sparsity. The remaining nonzero weights are then fine-tuned onto training dataset.\n",
    "\n",
    "After we prune the model, we will apply QAT to convert the weights from FP32 to INT8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8447d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, load the trained model from Part 3\n",
    "checkpoint = torch.load(\"./densenet-models/training/dense-model.pth\")\n",
    "model = torchvision.models.densenet121()\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, NUM_LABELS)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.to(device)\n",
    "\n",
    "# setup loss function and optimizer, LR will be overriden by sparseml\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=8e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a05d884",
   "metadata": {},
   "source": [
    "Next, we need to create a SparseML recipe which includes the GMP algorithm. The `!GlobalMagnitudePruningModifier` modifier instructs SparseML to apply the GMP algorithm at a global level (pruning the lowest magnitude weights across all layers).\n",
    "\n",
    "Firstly, we need to decide identify which parameters of the model to apply the GMP algorithm to. We can use the `get_prunable_layers` function to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d367905a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.conv0\n",
      "features.denseblock1.denselayer1.conv1\n",
      "features.denseblock1.denselayer1.conv2\n",
      "features.denseblock1.denselayer2.conv1\n",
      "features.denseblock1.denselayer2.conv2\n",
      "features.denseblock1.denselayer3.conv1\n",
      "features.denseblock1.denselayer3.conv2\n",
      "features.denseblock1.denselayer4.conv1\n",
      "features.denseblock1.denselayer4.conv2\n",
      "features.denseblock1.denselayer5.conv1\n",
      "features.denseblock1.denselayer5.conv2\n",
      "features.denseblock1.denselayer6.conv1\n",
      "features.denseblock1.denselayer6.conv2\n",
      "features.transition1.conv\n",
      "features.denseblock2.denselayer1.conv1\n",
      "features.denseblock2.denselayer1.conv2\n",
      "features.denseblock2.denselayer2.conv1\n",
      "features.denseblock2.denselayer2.conv2\n",
      "features.denseblock2.denselayer3.conv1\n",
      "features.denseblock2.denselayer3.conv2\n",
      "features.denseblock2.denselayer4.conv1\n",
      "features.denseblock2.denselayer4.conv2\n",
      "features.denseblock2.denselayer5.conv1\n",
      "features.denseblock2.denselayer5.conv2\n",
      "features.denseblock2.denselayer6.conv1\n",
      "features.denseblock2.denselayer6.conv2\n",
      "features.denseblock2.denselayer7.conv1\n",
      "features.denseblock2.denselayer7.conv2\n",
      "features.denseblock2.denselayer8.conv1\n",
      "features.denseblock2.denselayer8.conv2\n",
      "features.denseblock2.denselayer9.conv1\n",
      "features.denseblock2.denselayer9.conv2\n",
      "features.denseblock2.denselayer10.conv1\n",
      "features.denseblock2.denselayer10.conv2\n",
      "features.denseblock2.denselayer11.conv1\n",
      "features.denseblock2.denselayer11.conv2\n",
      "features.denseblock2.denselayer12.conv1\n",
      "features.denseblock2.denselayer12.conv2\n",
      "features.transition2.conv\n",
      "features.denseblock3.denselayer1.conv1\n",
      "features.denseblock3.denselayer1.conv2\n",
      "features.denseblock3.denselayer2.conv1\n",
      "features.denseblock3.denselayer2.conv2\n",
      "features.denseblock3.denselayer3.conv1\n",
      "features.denseblock3.denselayer3.conv2\n",
      "features.denseblock3.denselayer4.conv1\n",
      "features.denseblock3.denselayer4.conv2\n",
      "features.denseblock3.denselayer5.conv1\n",
      "features.denseblock3.denselayer5.conv2\n",
      "features.denseblock3.denselayer6.conv1\n",
      "features.denseblock3.denselayer6.conv2\n",
      "features.denseblock3.denselayer7.conv1\n",
      "features.denseblock3.denselayer7.conv2\n",
      "features.denseblock3.denselayer8.conv1\n",
      "features.denseblock3.denselayer8.conv2\n",
      "features.denseblock3.denselayer9.conv1\n",
      "features.denseblock3.denselayer9.conv2\n",
      "features.denseblock3.denselayer10.conv1\n",
      "features.denseblock3.denselayer10.conv2\n",
      "features.denseblock3.denselayer11.conv1\n",
      "features.denseblock3.denselayer11.conv2\n",
      "features.denseblock3.denselayer12.conv1\n",
      "features.denseblock3.denselayer12.conv2\n",
      "features.denseblock3.denselayer13.conv1\n",
      "features.denseblock3.denselayer13.conv2\n",
      "features.denseblock3.denselayer14.conv1\n",
      "features.denseblock3.denselayer14.conv2\n",
      "features.denseblock3.denselayer15.conv1\n",
      "features.denseblock3.denselayer15.conv2\n",
      "features.denseblock3.denselayer16.conv1\n",
      "features.denseblock3.denselayer16.conv2\n",
      "features.denseblock3.denselayer17.conv1\n",
      "features.denseblock3.denselayer17.conv2\n",
      "features.denseblock3.denselayer18.conv1\n",
      "features.denseblock3.denselayer18.conv2\n",
      "features.denseblock3.denselayer19.conv1\n",
      "features.denseblock3.denselayer19.conv2\n",
      "features.denseblock3.denselayer20.conv1\n",
      "features.denseblock3.denselayer20.conv2\n",
      "features.denseblock3.denselayer21.conv1\n",
      "features.denseblock3.denselayer21.conv2\n",
      "features.denseblock3.denselayer22.conv1\n",
      "features.denseblock3.denselayer22.conv2\n",
      "features.denseblock3.denselayer23.conv1\n",
      "features.denseblock3.denselayer23.conv2\n",
      "features.denseblock3.denselayer24.conv1\n",
      "features.denseblock3.denselayer24.conv2\n",
      "features.transition3.conv\n",
      "features.denseblock4.denselayer1.conv1\n",
      "features.denseblock4.denselayer1.conv2\n",
      "features.denseblock4.denselayer2.conv1\n",
      "features.denseblock4.denselayer2.conv2\n",
      "features.denseblock4.denselayer3.conv1\n",
      "features.denseblock4.denselayer3.conv2\n",
      "features.denseblock4.denselayer4.conv1\n",
      "features.denseblock4.denselayer4.conv2\n",
      "features.denseblock4.denselayer5.conv1\n",
      "features.denseblock4.denselayer5.conv2\n",
      "features.denseblock4.denselayer6.conv1\n",
      "features.denseblock4.denselayer6.conv2\n",
      "features.denseblock4.denselayer7.conv1\n",
      "features.denseblock4.denselayer7.conv2\n",
      "features.denseblock4.denselayer8.conv1\n",
      "features.denseblock4.denselayer8.conv2\n",
      "features.denseblock4.denselayer9.conv1\n",
      "features.denseblock4.denselayer9.conv2\n",
      "features.denseblock4.denselayer10.conv1\n",
      "features.denseblock4.denselayer10.conv2\n",
      "features.denseblock4.denselayer11.conv1\n",
      "features.denseblock4.denselayer11.conv2\n",
      "features.denseblock4.denselayer12.conv1\n",
      "features.denseblock4.denselayer12.conv2\n",
      "features.denseblock4.denselayer13.conv1\n",
      "features.denseblock4.denselayer13.conv2\n",
      "features.denseblock4.denselayer14.conv1\n",
      "features.denseblock4.denselayer14.conv2\n",
      "features.denseblock4.denselayer15.conv1\n",
      "features.denseblock4.denselayer15.conv2\n",
      "features.denseblock4.denselayer16.conv1\n",
      "features.denseblock4.denselayer16.conv2\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "# print parameters\n",
    "for (name, layer) in get_prunable_layers(model):\n",
    "    print(f\"{name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322fa136",
   "metadata": {},
   "source": [
    "We will apply GMP to all layers with `__ALL_PRUNABLE__`. Here is what the recipe looks like:\n",
    "\n",
    "```yaml\n",
    "# Epoch hyperparams\n",
    "stabilization_epochs: 1.0\n",
    "pruning_epochs: 9.0\n",
    "finetuning_epochs: 5.0\n",
    "quantization_epochs: 3.0\n",
    "\n",
    "# Learning rate hyperparams\n",
    "init_lr: 0.0001\n",
    "final_lr: 0.00005\n",
    "\n",
    "# Pruning hyperparams\n",
    "init_sparsity: 0.05\n",
    "final_sparsity: 0.9\n",
    "\n",
    "# Stabalization Stage\n",
    "training_modifiers:\n",
    "  - !EpochRangeModifier\n",
    "    start_epoch: 0.0\n",
    "    end_epoch: eval(stabilization_epochs + pruning_epochs + finetuning_epochs + quantization_epochs)\n",
    "\n",
    "  - !SetLearningRateModifier\n",
    "    start_epoch: 0.0\n",
    "    learning_rate: eval(init_lr)\n",
    "\n",
    "# Pruning Stage\n",
    "pruning_modifiers:\n",
    "  - !LearningRateFunctionModifier\n",
    "    init_lr: eval(init_lr)\n",
    "    final_lr: eval(final_lr)\n",
    "    lr_func: cosine\n",
    "    start_epoch: eval(stabilization_epochs)\n",
    "    end_epoch: eval(stabilization_epochs + pruning_epochs)\n",
    "\n",
    "  - !GlobalMagnitudePruningModifier\n",
    "    init_sparsity: eval(init_sparsity)\n",
    "    final_sparsity: eval(final_sparsity)\n",
    "    start_epoch: eval(stabilization_epochs)\n",
    "    end_epoch: eval(stabilization_epochs + pruning_epochs)\n",
    "    update_frequency: 0.5\n",
    "    params: __ALL_PRUNABLE__\n",
    "    leave_enabled: True\n",
    "\n",
    "# Finetuning Stage\n",
    "finetuning_modifiers:\n",
    "  - !LearningRateFunctionModifier\n",
    "    init_lr: eval(init_lr)\n",
    "    final_lr: eval(final_lr)\n",
    "    lr_func: cosine\n",
    "    start_epoch: eval(stabilization_epochs + pruning_epochs)\n",
    "    end_epoch: eval(stabilization_epochs + pruning_epochs + finetuning_epochs)\n",
    "\n",
    "# Quantization hyperparams\n",
    "quantization_modifiers:\n",
    "  - !QuantizationModifier\n",
    "    start_epoch: eval(stabilization_epochs + pruning_epochs + finetuning_epochs)\n",
    "```\n",
    "\n",
    "This recipe specifies that we will run the GMP algorithm for the first 10 epochs. We start at an `init_sparsity` level of 5% and gradually increase sparsity to a `final_sparsity` level of 90% following a `cubic` curve across each of the layers in the network.\n",
    "\n",
    "Over the next 5 epochs, we fine-tune the 90% pruned model further. Since we set `leave_enabled=True` the sparsity level will be maintained as the fine-tuning occurs.\n",
    "\n",
    "Over the final 3 epochs, we apply QAT to quantize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8518b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_recipe_path = \"./recipe.prune_quant.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b17fc31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Epoch hyperparams\r\n",
      "stabilization_epochs: 1.0\r\n",
      "pruning_epochs: 9.0\r\n",
      "finetuning_epochs: 5.0\r\n",
      "quantization_epochs: 3.0\r\n",
      "\r\n",
      "# Learning rate hyperparams\r\n",
      "init_lr: 0.0001\r\n",
      "final_lr: 0.00005\r\n",
      "\r\n",
      "# Pruning hyperparams\r\n",
      "init_sparsity: 0.05\r\n",
      "final_sparsity: 0.9\r\n",
      "\r\n",
      "# Stabalization Stage\r\n",
      "training_modifiers:\r\n",
      "  - !EpochRangeModifier\r\n",
      "    start_epoch: 0.0\r\n",
      "    end_epoch: eval(stabilization_epochs + pruning_epochs + finetuning_epochs + quantization_epochs)\r\n",
      "\r\n",
      "  - !SetLearningRateModifier\r\n",
      "    start_epoch: 0.0\r\n",
      "    learning_rate: eval(init_lr)\r\n",
      "\r\n",
      "# Pruning Stage\r\n",
      "pruning_modifiers:\r\n",
      "  - !LearningRateFunctionModifier\r\n",
      "    init_lr: eval(init_lr)\r\n",
      "    final_lr: eval(final_lr)\r\n",
      "    lr_func: cosine\r\n",
      "    start_epoch: eval(stabilization_epochs)\r\n",
      "    end_epoch: eval(stabilization_epochs + pruning_epochs)\r\n",
      "\r\n",
      "  - !GlobalMagnitudePruningModifier\r\n",
      "    init_sparsity: eval(init_sparsity)\r\n",
      "    final_sparsity: eval(final_sparsity)\r\n",
      "    start_epoch: eval(stabilization_epochs)\r\n",
      "    end_epoch: eval(stabilization_epochs + pruning_epochs)\r\n",
      "    update_frequency: 0.5\r\n",
      "    params: __ALL_PRUNABLE__\r\n",
      "    leave_enabled: True\r\n",
      "\r\n",
      "# Finetuning Stage\r\n",
      "finetuning_modifiers:\r\n",
      "  - !LearningRateFunctionModifier\r\n",
      "    init_lr: eval(init_lr)\r\n",
      "    final_lr: eval(final_lr)\r\n",
      "    lr_func: cosine\r\n",
      "    start_epoch: eval(stabilization_epochs + pruning_epochs)\r\n",
      "    end_epoch: eval(stabilization_epochs + pruning_epochs + finetuning_epochs)\r\n",
      "\r\n",
      "# Quantization hyperparams\r\n",
      "quantization_modifiers:\r\n",
      "  - !QuantizationModifier\r\n",
      "    start_epoch: eval(stabilization_epochs + pruning_epochs + finetuning_epochs)"
     ]
    }
   ],
   "source": [
    "!cat ./recipe.prune_quant.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68646bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ScheduledModifierManager and Optimizer wrapper\n",
    "manager = ScheduledModifierManager.from_yaml(pruning_recipe_path)\n",
    "logger = TensorBoardLogger(log_path=\"./tensorboard_outputs/densenet/pruning-run\")\n",
    "optimizer = manager.modify(model, optimizer, loggers=[logger], steps_per_epoch=len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0a5909",
   "metadata": {},
   "source": [
    "Next, kick off the GMP training loop. \n",
    "\n",
    "As you can see, we use the wrapped `optimizer` and `model` in the same way as above. SparseML parsed the recipe and updated the `optimizer` with the logic of GMP algorithm from the recipe. This allows you to use the `optimizer` and `model` as usual, with all of the pruning-related logic handled by SparseML.\n",
    "\n",
    "Our 90% pruned model reaches ~90% validation accuracy (vs ~90% for the dense model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "601c8c21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Training Epoch 1/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e21a6ad014c4baa9cd1defc0cc6b045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1/18\n",
      "Training Loss: 0.01275764120509848\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 1/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e233ff551474ef3975a3b21338a9129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 1/18\n",
      "Val Loss: 0.3881779580115108\n",
      "Top 1 Acc: 0.9009803921568628\n",
      "\n",
      "Running Training Epoch 2/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0a719636454ab380355b13ff82f692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2/18\n",
      "Training Loss: 0.006604243415495148\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 2/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a13e006cb5f448f989961261a345cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 2/18\n",
      "Val Loss: 0.36990982507995795\n",
      "Top 1 Acc: 0.9019607843137255\n",
      "\n",
      "Running Training Epoch 3/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00a695d9d10432ea6f901642635adfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3/18\n",
      "Training Loss: 0.007901180022599874\n",
      "Top 1 Acc: 0.9990196078431373\n",
      "\n",
      "Running Validation Epoch 3/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9381f42611fe444c8ed6ce3587a0c69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 3/18\n",
      "Val Loss: 0.36820574732155364\n",
      "Top 1 Acc: 0.9029411764705882\n",
      "\n",
      "Running Training Epoch 4/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee235f33fd044de7aefcdedd45831595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4/18\n",
      "Training Loss: 0.00951768000231823\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 4/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d242fb0e66fa492c96a7590927e19803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 4/18\n",
      "Val Loss: 0.35841650055590435\n",
      "Top 1 Acc: 0.9137254901960784\n",
      "\n",
      "Running Training Epoch 5/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc90664647a94763b6a4eaff79f8e27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5/18\n",
      "Training Loss: 0.017474219625000842\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 5/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c7f844da9c49a7872a3a7f66fd3500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 5/18\n",
      "Val Loss: 0.39351300171983894\n",
      "Top 1 Acc: 0.907843137254902\n",
      "\n",
      "Running Training Epoch 6/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8ccda7ee3e42a9a25e714bc96fce4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6/18\n",
      "Training Loss: 0.043898234391235746\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 6/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c0255c208342d692b0e8b04233f9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 6/18\n",
      "Val Loss: 0.41797342558857054\n",
      "Top 1 Acc: 0.9058823529411765\n",
      "\n",
      "Running Training Epoch 7/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fac72ecd3b548d78b03eb08a065855f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7/18\n",
      "Training Loss: 0.09149695426458493\n",
      "Top 1 Acc: 0.9990196078431373\n",
      "\n",
      "Running Validation Epoch 7/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc99f73dcd66413ea9c98c37a4bf9e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 7/18\n",
      "Val Loss: 0.4712565544323297\n",
      "Top 1 Acc: 0.8931372549019608\n",
      "\n",
      "Running Training Epoch 8/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b8c73168104605ab70886f6082b36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8/18\n",
      "Training Loss: 0.1224245154298842\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 8/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f985542f2e8476ca7038c24d1083440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 8/18\n",
      "Val Loss: 0.5245802142017055\n",
      "Top 1 Acc: 0.8921568627450981\n",
      "\n",
      "Running Training Epoch 9/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f90817107a457e8c77c63a5135fba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9/18\n",
      "Training Loss: 0.12702065438497812\n",
      "Top 1 Acc: 0.9980392156862745\n",
      "\n",
      "Running Validation Epoch 9/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef978e2ec05c4b4a9d84cf0c86d8cb2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 9/18\n",
      "Val Loss: 0.5165597766754217\n",
      "Top 1 Acc: 0.8980392156862745\n",
      "\n",
      "Running Training Epoch 10/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9548e1ae8c314ff9b4c271de9eed42a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10/18\n",
      "Training Loss: 0.08719327434664592\n",
      "Top 1 Acc: 0.9990196078431373\n",
      "\n",
      "Running Validation Epoch 10/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3c9cdefd604243b35a33684a35d278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 10/18\n",
      "Val Loss: 0.48045873222872615\n",
      "Top 1 Acc: 0.907843137254902\n",
      "\n",
      "Running Training Epoch 11/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ff9c975c77493197506168b32cafe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11/18\n",
      "Training Loss: 0.06748799287015572\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 11/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a5eab8829042018d6989c5bee74725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 11/18\n",
      "Val Loss: 0.4438641173474025\n",
      "Top 1 Acc: 0.9137254901960784\n",
      "\n",
      "Running Training Epoch 12/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354919de108646379da4b883a93a6493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12/18\n",
      "Training Loss: 0.050291253632167354\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 12/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b317639f0148fcaf00722c285e9933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 12/18\n",
      "Val Loss: 0.43283609559875913\n",
      "Top 1 Acc: 0.9107843137254902\n",
      "\n",
      "Running Training Epoch 13/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b9ff025b644b7d82321d7267cf1ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13/18\n",
      "Training Loss: 0.0417996046890039\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 13/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0431a24f22344020893c819efeb8d780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 13/18\n",
      "Val Loss: 0.42522090824786574\n",
      "Top 1 Acc: 0.9088235294117647\n",
      "\n",
      "Running Training Epoch 14/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc188fd681f4912be872bea88b9ace8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14/18\n",
      "Training Loss: 0.03894200167269446\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 14/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a2c480fcac4ff8ab6c215bf4742faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 14/18\n",
      "Val Loss: 0.42405990276893135\n",
      "Top 1 Acc: 0.9098039215686274\n",
      "\n",
      "Running Training Epoch 15/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013d0ca8889e4d418fb3e7b016afc6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15/18\n",
      "Training Loss: 0.03521087218541652\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 15/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84731f17bec64dcea1842d932e40916b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 15/18\n",
      "Val Loss: 0.42383157154836226\n",
      "Top 1 Acc: 0.9117647058823529\n",
      "\n",
      "Running Training Epoch 16/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a28196d57041c186846e9d90801bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 16/18\n",
      "Training Loss: 0.03761145524913445\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 16/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7004f2fca164ce88db1c69704e492f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 16/18\n",
      "Val Loss: 0.41609785216860473\n",
      "Top 1 Acc: 0.9127450980392157\n",
      "\n",
      "Running Training Epoch 17/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709b3ab3f7fb4f8da9ffa47a035f7f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 17/18\n",
      "Training Loss: 0.0321939253481105\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 17/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfffd84de1824e5e8d9e8f415a19d9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 17/18\n",
      "Val Loss: 0.42837292490003165\n",
      "Top 1 Acc: 0.9058823529411765\n",
      "\n",
      "Running Training Epoch 18/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe508bb97d04b92b71dfd45491c4052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 18/18\n",
      "Training Loss: 0.03206482196401339\n",
      "Top 1 Acc: 1.0\n",
      "\n",
      "Running Validation Epoch 18/18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb3d70a35c84fc7b8813043c067c850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch: 18/18\n",
      "Val Loss: 0.4149022806814173\n",
      "Top 1 Acc: 0.9107843137254902\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run GMP algorithm\n",
    "epoch = 0\n",
    "for epoch in range(manager.max_epochs):\n",
    "    # run training loop\n",
    "    epoch_name = f\"{epoch + 1}/{manager.max_epochs}\"\n",
    "    \n",
    "    print(f\"Running Training Epoch {epoch_name}\")\n",
    "    train_loss, train_acc = run_model_one_epoch(model, train_loader, criterion, device, train=True, optimizer=optimizer)\n",
    "    print(f\"Training Epoch: {epoch_name}\\nTraining Loss: {train_loss}\\nTop 1 Acc: {train_acc}\\n\")\n",
    "\n",
    "    # run validation loop\n",
    "    print(f\"Running Validation Epoch {epoch_name}\")\n",
    "    val_loss, val_acc = run_model_one_epoch(model, val_loader, criterion, device)\n",
    "    print(f\"Validation Epoch: {epoch_name}\\nVal Loss: {val_loss}\\nTop 1 Acc: {val_acc}\\n\")\n",
    "    \n",
    "    logger.log_scalar(\"Metrics/Loss (Train)\", train_loss, epoch)\n",
    "    logger.log_scalar(\"Metrics/Accuracy (Train)\", train_acc, epoch)\n",
    "    logger.log_scalar(\"Metrics/Loss (Validation)\", val_loss, epoch)\n",
    "    logger.log_scalar(\"Metrics/Accuracy (Validation)\", val_acc, epoch)\n",
    "\n",
    "manager.finalize(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c96232",
   "metadata": {},
   "source": [
    "The resulting model is is 90% sparse and quantized, while achieving validation accuracy of ~91% (vs the unoptimized dense model at ~91%) without much hyperparameter search. Key hyperparameter experiments you may want to run include:\n",
    "- Learning rate\n",
    "- Learning rate schedule\n",
    "- Sparsity level\n",
    "- Number of pruning epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df5afa94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity By Layer:\n",
      "features.conv0.module.weight: 0.4833\n",
      "features.denseblock1.denselayer1.conv1.module.weight: 0.7510\n",
      "features.denseblock1.denselayer1.conv2.module.weight: 0.8344\n",
      "features.denseblock1.denselayer2.conv1.module.weight: 0.7566\n",
      "features.denseblock1.denselayer2.conv2.module.weight: 0.8695\n",
      "features.denseblock1.denselayer3.conv1.module.weight: 0.7657\n",
      "features.denseblock1.denselayer3.conv2.module.weight: 0.8344\n",
      "features.denseblock1.denselayer4.conv1.module.weight: 0.8263\n",
      "features.denseblock1.denselayer4.conv2.module.weight: 0.8338\n",
      "features.denseblock1.denselayer5.conv1.module.weight: 0.8754\n",
      "features.denseblock1.denselayer5.conv2.module.weight: 0.8872\n",
      "features.denseblock1.denselayer6.conv1.module.weight: 0.8493\n",
      "features.denseblock1.denselayer6.conv2.module.weight: 0.8465\n",
      "features.transition1.conv.module.weight: 0.7229\n",
      "features.denseblock2.denselayer1.conv1.module.weight: 0.9299\n",
      "features.denseblock2.denselayer1.conv2.module.weight: 0.8891\n",
      "features.denseblock2.denselayer2.conv1.module.weight: 0.8853\n",
      "features.denseblock2.denselayer2.conv2.module.weight: 0.8640\n",
      "features.denseblock2.denselayer3.conv1.module.weight: 0.8879\n",
      "features.denseblock2.denselayer3.conv2.module.weight: 0.8501\n",
      "features.denseblock2.denselayer4.conv1.module.weight: 0.8690\n",
      "features.denseblock2.denselayer4.conv2.module.weight: 0.8710\n",
      "features.denseblock2.denselayer5.conv1.module.weight: 0.8941\n",
      "features.denseblock2.denselayer5.conv2.module.weight: 0.8551\n",
      "features.denseblock2.denselayer6.conv1.module.weight: 0.9125\n",
      "features.denseblock2.denselayer6.conv2.module.weight: 0.8675\n",
      "features.denseblock2.denselayer7.conv1.module.weight: 0.8842\n",
      "features.denseblock2.denselayer7.conv2.module.weight: 0.8809\n",
      "features.denseblock2.denselayer8.conv1.module.weight: 0.8934\n",
      "features.denseblock2.denselayer8.conv2.module.weight: 0.8659\n",
      "features.denseblock2.denselayer9.conv1.module.weight: 0.9026\n",
      "features.denseblock2.denselayer9.conv2.module.weight: 0.8797\n",
      "features.denseblock2.denselayer10.conv1.module.weight: 0.8774\n",
      "features.denseblock2.denselayer10.conv2.module.weight: 0.8845\n",
      "features.denseblock2.denselayer11.conv1.module.weight: 0.8886\n",
      "features.denseblock2.denselayer11.conv2.module.weight: 0.8872\n",
      "features.denseblock2.denselayer12.conv1.module.weight: 0.8787\n",
      "features.denseblock2.denselayer12.conv2.module.weight: 0.9016\n",
      "features.transition2.conv.module.weight: 0.7787\n",
      "features.denseblock3.denselayer1.conv1.module.weight: 0.9179\n",
      "features.denseblock3.denselayer1.conv2.module.weight: 0.9011\n",
      "features.denseblock3.denselayer2.conv1.module.weight: 0.9384\n",
      "features.denseblock3.denselayer2.conv2.module.weight: 0.9034\n",
      "features.denseblock3.denselayer3.conv1.module.weight: 0.9201\n",
      "features.denseblock3.denselayer3.conv2.module.weight: 0.9031\n",
      "features.denseblock3.denselayer4.conv1.module.weight: 0.9208\n",
      "features.denseblock3.denselayer4.conv2.module.weight: 0.8861\n",
      "features.denseblock3.denselayer5.conv1.module.weight: 0.9354\n",
      "features.denseblock3.denselayer5.conv2.module.weight: 0.8859\n",
      "features.denseblock3.denselayer6.conv1.module.weight: 0.9218\n",
      "features.denseblock3.denselayer6.conv2.module.weight: 0.8845\n",
      "features.denseblock3.denselayer7.conv1.module.weight: 0.9403\n",
      "features.denseblock3.denselayer7.conv2.module.weight: 0.8794\n",
      "features.denseblock3.denselayer8.conv1.module.weight: 0.9099\n",
      "features.denseblock3.denselayer8.conv2.module.weight: 0.8889\n",
      "features.denseblock3.denselayer9.conv1.module.weight: 0.9145\n",
      "features.denseblock3.denselayer9.conv2.module.weight: 0.8837\n",
      "features.denseblock3.denselayer10.conv1.module.weight: 0.9424\n",
      "features.denseblock3.denselayer10.conv2.module.weight: 0.8882\n",
      "features.denseblock3.denselayer11.conv1.module.weight: 0.9370\n",
      "features.denseblock3.denselayer11.conv2.module.weight: 0.8742\n",
      "features.denseblock3.denselayer12.conv1.module.weight: 0.9259\n",
      "features.denseblock3.denselayer12.conv2.module.weight: 0.8923\n",
      "features.denseblock3.denselayer13.conv1.module.weight: 0.9396\n",
      "features.denseblock3.denselayer13.conv2.module.weight: 0.8650\n",
      "features.denseblock3.denselayer14.conv1.module.weight: 0.9456\n",
      "features.denseblock3.denselayer14.conv2.module.weight: 0.8813\n",
      "features.denseblock3.denselayer15.conv1.module.weight: 0.9254\n",
      "features.denseblock3.denselayer15.conv2.module.weight: 0.8867\n",
      "features.denseblock3.denselayer16.conv1.module.weight: 0.9379\n",
      "features.denseblock3.denselayer16.conv2.module.weight: 0.9118\n",
      "features.denseblock3.denselayer17.conv1.module.weight: 0.9204\n",
      "features.denseblock3.denselayer17.conv2.module.weight: 0.8989\n",
      "features.denseblock3.denselayer18.conv1.module.weight: 0.9312\n",
      "features.denseblock3.denselayer18.conv2.module.weight: 0.8978\n",
      "features.denseblock3.denselayer19.conv1.module.weight: 0.9254\n",
      "features.denseblock3.denselayer19.conv2.module.weight: 0.8973\n",
      "features.denseblock3.denselayer20.conv1.module.weight: 0.9275\n",
      "features.denseblock3.denselayer20.conv2.module.weight: 0.9037\n",
      "features.denseblock3.denselayer21.conv1.module.weight: 0.9226\n",
      "features.denseblock3.denselayer21.conv2.module.weight: 0.8967\n",
      "features.denseblock3.denselayer22.conv1.module.weight: 0.9238\n",
      "features.denseblock3.denselayer22.conv2.module.weight: 0.8761\n",
      "features.denseblock3.denselayer23.conv1.module.weight: 0.9235\n",
      "features.denseblock3.denselayer23.conv2.module.weight: 0.9083\n",
      "features.denseblock3.denselayer24.conv1.module.weight: 0.9279\n",
      "features.denseblock3.denselayer24.conv2.module.weight: 0.9075\n",
      "features.transition3.conv.module.weight: 0.8501\n",
      "features.denseblock4.denselayer1.conv1.module.weight: 0.8785\n",
      "features.denseblock4.denselayer1.conv2.module.weight: 0.9120\n",
      "features.denseblock4.denselayer2.conv1.module.weight: 0.8744\n",
      "features.denseblock4.denselayer2.conv2.module.weight: 0.9431\n",
      "features.denseblock4.denselayer3.conv1.module.weight: 0.8868\n",
      "features.denseblock4.denselayer3.conv2.module.weight: 0.9228\n",
      "features.denseblock4.denselayer4.conv1.module.weight: 0.9035\n",
      "features.denseblock4.denselayer4.conv2.module.weight: 0.9385\n",
      "features.denseblock4.denselayer5.conv1.module.weight: 0.9001\n",
      "features.denseblock4.denselayer5.conv2.module.weight: 0.9546\n",
      "features.denseblock4.denselayer6.conv1.module.weight: 0.8989\n",
      "features.denseblock4.denselayer6.conv2.module.weight: 0.9562\n",
      "features.denseblock4.denselayer7.conv1.module.weight: 0.9099\n",
      "features.denseblock4.denselayer7.conv2.module.weight: 0.9548\n",
      "features.denseblock4.denselayer8.conv1.module.weight: 0.9137\n",
      "features.denseblock4.denselayer8.conv2.module.weight: 0.9547\n",
      "features.denseblock4.denselayer9.conv1.module.weight: 0.9150\n",
      "features.denseblock4.denselayer9.conv2.module.weight: 0.9614\n",
      "features.denseblock4.denselayer10.conv1.module.weight: 0.9196\n",
      "features.denseblock4.denselayer10.conv2.module.weight: 0.9607\n",
      "features.denseblock4.denselayer11.conv1.module.weight: 0.9183\n",
      "features.denseblock4.denselayer11.conv2.module.weight: 0.9591\n",
      "features.denseblock4.denselayer12.conv1.module.weight: 0.9233\n",
      "features.denseblock4.denselayer12.conv2.module.weight: 0.9610\n",
      "features.denseblock4.denselayer13.conv1.module.weight: 0.9242\n",
      "features.denseblock4.denselayer13.conv2.module.weight: 0.9595\n",
      "features.denseblock4.denselayer14.conv1.module.weight: 0.9280\n",
      "features.denseblock4.denselayer14.conv2.module.weight: 0.9607\n",
      "features.denseblock4.denselayer15.conv1.module.weight: 0.9294\n",
      "features.denseblock4.denselayer15.conv2.module.weight: 0.9594\n",
      "features.denseblock4.denselayer16.conv1.module.weight: 0.9253\n",
      "features.denseblock4.denselayer16.conv2.module.weight: 0.9558\n",
      "classifier.module.weight: 0.8173\n"
     ]
    }
   ],
   "source": [
    "print(\"Sparsity By Layer:\")\n",
    "for (name, layer) in get_prunable_layers(model):\n",
    "    print(f\"{name}.weight: {tensor_sparsity(layer.weight).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44fb7550",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"densenet-models\"\n",
    "exporter = ModuleExporter(model, output_dir=save_dir)\n",
    "exporter.export_pytorch(name=\"densenet-pruned-int8.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
